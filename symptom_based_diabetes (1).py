# -*- coding: utf-8 -*-
"""Symptom based diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_h9q6fGiBIsxsa9H5T_OM95pnYvs5NVY
"""

from google.colab import drive
drive.mount('/content/drive')

# For ignoring the warnings
import warnings
warnings.filterwarnings('ignore')

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Importing tha data
import pandas as pd
dataset = pd.read_csv('/content/drive/MyDrive/Project Databases/1.Diabetes symptoms data.csv')
dataset.head()

dataset.isnull().sum() # For null values

x = dataset.iloc[:,:-1]  # Data except last column
y = dataset.iloc[:,-1] # last column in data
x.head()

y.head()

dataset.corr() # Calculating the relationship between each column in dataset

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 15))  # Adjust the figsize as per your preference
sns.heatmap(dataset.corr(), fmt='.0%', annot=True)

plt.show()         # colored visual summary of information

import matplotlib.pyplot as plt

fig, ax = plt.subplots(nrows=7, ncols=2, figsize=(12, 10))
fig.tight_layout(pad=3.0)
ax[0,0].set_title('irritability')
ax[0,0].hist(dataset.irritability[dataset.Outcome==1]);
ax[0,1].set_title('polyuria')
ax[0,1].hist(dataset.polyuria[dataset.Outcome==1]);
ax[1,0].set_title('polydipsia')
ax[1,0].hist(dataset.polydipsia[dataset.Outcome==1]);
ax[1,1].set_title('sudden_weight_loss')
ax[1,1].hist(dataset.sudden_weight_loss[dataset.Outcome==1]);
ax[2,0].set_title('weakness')
ax[2,0].hist(dataset.weakness[dataset.Outcome==1]);
ax[2,1].set_title('polyphagia')
ax[2,1].hist(dataset.polyphagia[dataset.Outcome==1]);
ax[3,0].set_title('genital_thrush')
ax[3,0].hist(dataset.genital_thrush[dataset.Outcome==1]);
ax[3,1].set_title('visual_blurring')
ax[3,1].hist(dataset.visual_blurring[dataset.Outcome==1])
ax[4,0].set_title('itching')
ax[4,0].hist(dataset.itching[dataset.Outcome==1])
ax[4,1].set_title('delayed_healing')
ax[4,1].hist(dataset.delayed_healing[dataset.Outcome==1])
ax[5,0].set_title('partial_paresis')
ax[5,0].hist(dataset.partial_paresis[dataset.Outcome==1])
ax[5,1].set_title('muscle_stiffness')
ax[5,1].hist(dataset.muscle_stiffness[dataset.Outcome==1])
ax[6,0].set_title('alopecia')
ax[6,0].hist(dataset.alopecia[dataset.Outcome==1])
ax[6,1].set_title('obesity')
ax[6,1].hist(dataset.obesity[dataset.Outcome==1])

import seaborn as sns
import matplotlib.pyplot as plt

ax = sns.countplot(data=dataset, x='Outcome')

# Add numbers on top of the bars
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')

plt.show()

## Pairplotting of dataframe
import seaborn as sns
sns.set(style="ticks", color_codes=True)
sns.pairplot(dataset,hue='Outcome',palette='gnuplot');

# For training and testing of model

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1)

x_train['gender'] = x_train['gender'].map({'Female': 0, 'Male': 1})

x_test['gender'] = x_test['gender'].map({'Female': 0, 'Male': 1})

""" **K NEAREST NEIGHBOR MODEL**"""

# K nearest neighbor model

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 23)
knn.fit(x_train,y_train)
predict1 = knn.predict(x_test)
predict1

y_test # Testing data of y

# For counting mis-classifies values

count_missclassified1 = (y_test!=predict1).sum()
count_missclassified1

# For finding the accuracy

from sklearn import metrics
accuracy1 = metrics.accuracy_score(y_test,predict1)
accuracy1*100

# Clssification report

c_f1 = metrics.classification_report(y_test,predict1)
print(c_f1)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your model
predictions = knn.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = knn.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the value

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred1 = knn.predict(n)
pred1

"""**Logistic regression**"""

# Analysing with Logistic regression

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=20,penalty='l2')
lr.fit(x_train,y_train)
predict2 = lr.predict(x_test)
predict2

# Counting mis-classified values for logistic regression

count_missclassified2 = (y_test!=predict2).sum()
count_missclassified2

# For finding the accuracy

accuracy2 = metrics.accuracy_score(y_test,predict2)
accuracy2*100

# Evaluating classification report
c_r2 = metrics.classification_report(y_test,predict2)
print(c_r2)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your model
predictions = lr.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = lr.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred2 = lr.predict(n)
pred2

"""**Random Forest Algorithm**"""

#Random Forest Algorithm

from sklearn import ensemble
rf = ensemble.RandomForestClassifier(n_estimators = 7,criterion = 'entropy',max_features = 5,random_state=20) #random forest model
rf.fit(x_train,y_train)
predict3 = rf.predict(x_test) #prediction
predict3

# Counting mis-classified values for logistic regression

count_missclassified3 = (y_test!=predict3).sum()
count_missclassified3

accuracy3 = metrics.accuracy_score(y_test,predict3)
accuracy3*100

# Evaluating classification report
c_r3 = metrics.classification_report(y_test,predict3)
print(c_r3)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your model
predictions = rf.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = rf.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred3 = rf.predict(n)
pred3

"""**support vector machine SVM**"""

# support vector machine SVM
from sklearn import svm
#svc-> classificational algorithm

model = svm.SVC(probability=True)
model.fit(x_train,y_train)
predict4=model.predict(x_test)
predict4

# Counting mis-classified values for support vector machine SVM

count_missclassified4 = (y_test != predict4).sum()
count_missclassified4

#accuracy for support vector machine SVM
accuracy4=metrics.accuracy_score(y_test,predict4)
accuracy4*100

# Clssification report support vector machine SVM

c_r4 = metrics.classification_report(y_test,predict4)
print(c_r4)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your SVM model
predictions = model.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = model.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred4 = model.predict(n)
pred4

"""**Decision Tree model**"""

# Using Decision Tree model

from sklearn import tree
dtree = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=1,min_samples_split=7)
dtree.fit(x_train,y_train)
predict5 = dtree.predict(x_test)
predict5

# Counting mis-classified values for Decision Tree Algorithm

count_missclassified5 = (y_test != predict5).sum()
count_missclassified5

# Finding the accuracy

accuracy5 = metrics.accuracy_score(y_test,predict5)
accuracy5*100

# Clssification report

c_r5 = metrics.classification_report(y_test,predict5)
print(c_r5)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your SVM model
predictions = dtree.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = dtree.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred5 = dtree.predict(n)
pred5

"""**XG Boost**"""

#Using XG Boost
import xgboost as xgb
model2 = xgb.XGBClassifier(n_estimators=16, learning_rate=0.01, max_depth=3)
model2.fit(x_train, y_train)
predict6 = model2.predict(x_test)
predict6

#No. Of Missclassified
count_missclassified6 = (y_test != predict6).sum()
count_missclassified6

#calculating accuracy score
accuracy6 = metrics.accuracy_score(y_test,predict6)
accuracy6*100

# Clssification report

c_r6 = metrics.classification_report(y_test,predict6)
print(c_r6)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your SVM model
predictions = model2.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = model2.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred6 = model2.predict(n)
pred6

"""**Gaussian Naïve Bayes Classifier**"""

#Using Gaussian Naïve Bayes Classifier
from sklearn.naive_bayes import GaussianNB
model3 = GaussianNB()
model3.fit(x_train, y_train)
predict7 = model3.predict(x_test)
print(predict7)

# Counting mis-classified values for Gaussian Naïve Bayes Classifier

count_missclassified7 = (y_test != predict7).sum()
count_missclassified7

#calculating accuracy score
accuracy7 = metrics.accuracy_score(y_test,predict7)
accuracy7*100

# Clssification report

c_r7 = metrics.classification_report(y_test,predict7)
print(c_r7)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your SVM model
predictions = model3.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = model3.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred7 = model3.predict(n)
pred7

"""**ADA BOOST**"""

#using ada boost
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
model4 = AdaBoostClassifier(n_estimators=10, learning_rate=0.01, random_state=0)
model4.fit(x_train, y_train)
predict8 = model4.predict(x_test)
print(predict8)

# Counting mis-classified values for ADA Boost

count_missclassified8 = (y_test != predict8).sum()
count_missclassified8

#calculating accuracy score
accuracy8 = metrics.accuracy_score(y_test,predict8)
accuracy8*100

# Clssification report

c_r8 = metrics.classification_report(y_test,predict8)
print(c_r8)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your SVM model
predictions = model4.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = model4.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred8 = model4.predict(n)
pred8

"""**Gradient Boosted Model**"""

#Gradient Boosted Model
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
model5 = GradientBoostingClassifier(n_estimators=10, learning_rate=0.01, random_state=0)
model5.fit(x_train, y_train)
predict9 = model5.predict(x_test)
print(predict9)

# Counting mis-classified values for Gradient Boosted Model

count_missclassified8 = (y_test != predict9).sum()
count_missclassified8

#calculating accuracy score
accuracy9 = metrics.accuracy_score(y_test,predict9)
accuracy9*100

# Clssification report

c_r9 = metrics.classification_report(y_test,predict9)
print(c_r9)

from sklearn.metrics import confusion_matrix

# Obtain predictions from your SVM model
predictions = model5.predict(x_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predictions)

# Calculate sensitivity (true positive rate)
sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# Calculate specificity (true negative rate)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print sensitivity and specificity
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

from sklearn.metrics import cohen_kappa_score
predictions = model5.predict(x_test)
kappa = cohen_kappa_score(y_test, predictions)
print("Cohen's kappa value:", kappa)

# Testing with the values

n = [[40,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1]]
pred9 = model5.predict(n)
pred9

"""1D CNN"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

num_classes = 2

# Define the input shape
sequence_length =  # Replace with the actual length of your input sequences
input_dim =  # Replace with the actual number of input features

# Define the model
model6 = keras.Sequential()
model6.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(sequence_length, input_dim)))
model6.add(layers.MaxPooling1D(pool_size=2))
model6.add(layers.Flatten())
model6.add(layers.Dense(64, activation='relu'))
model6.add(layers.Dense(num_classes, activation='softmax'))

# Compile the model
model6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model6.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_val, y_val))

# Evaluate the model
score = model6.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

# Make predictions
predictions = model6.predict(x_test)

"""**COMPARISION**"""

import pandas as pd
model_compare = pd.DataFrame({"KNN":accuracy1*100,
"Logistic Regression":accuracy2*100,
"Random Forest Classifier":accuracy3*100,
"Support Vector Machine":accuracy4*100,
"Decision Tree Algorithm":accuracy5*100,
"XG Boost":accuracy6*100,
"Gaussian Naive Bayes":accuracy7*100,
"ADA Boost Model":accuracy8*100,
"Gradient Boosted Model":accuracy9*100},
index=["accuracy"])
model_compare.T.plot.bar(figsize=(15,10));

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for hyperparameter tuning
rf_params = {
    'n_estimators': [7, 14, 21],  # Number of trees in the forest
    'max_depth': [None, 5, 10],  # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node
}
rf = RandomForestClassifier()

clf = GridSearchCV(rf, rf_params, cv=5, verbose=True)

clf.fit(x_train, y_train)

best_params = clf.best_params_
score = clf.score(x_test, y_test)

print("Best Hyperparameters:", best_params)
print("Test Accuracy:", score)

y_prediction=clf.predict(x_test)
y_prediction

sns.set(font_scale=2)
import seaborn as sns
from sklearn.metrics import confusion_matrix
sns.heatmap(confusion_matrix(y_test,y_prediction), annot=True,cbar=False, fmt='g')
plt.xlabel("True label")
plt.ylabel("Predicted label");

# from sklearn.metrics import accuracy_score
print(metrics.accuracy_score(y_test,y_prediction))

print(metrics.classification_report(y_test, y_prediction))

from sklearn.metrics import RocCurveDisplay
RocCurveDisplay.from_estimator(clf,x_test,y_test)

import pickle
# Save trained model to file
pickle.dump(clf, open("Diabetes.pkl", "wb"))
loaded_model = pickle.load(open("Diabetes.pkl", "rb"))
loaded_model.predict(x_test)
loaded_model.score(x_test,y_test)
#######OUTPUT########